{
  "hash": "74f5ad65e2d7c6897a8a4f53ef98844f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Topic Modeling in R\"\neval: TRUE\n---\n\n\n\n## Clear Workspace, DON'T EDIT {-}\n\nAlways start by clearing the workspace. This ensure objects created in other files are not used used here.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list = ls())\n```\n:::\n\n\n\n## List Used Packages, EDIT {-}\n\nList all the packages that will be used in chunk below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npackages <- c(\"gutenbergr\", # download books from Project Gutenberg using book ID\n              \"tidyverse\",\n              \"tidytext\",\n              \"ggplot2\",\n              \"stm\", # for do topic modeling\n              \"quanteda\") # great text mining, will be used to structure the input to stm\n```\n:::\n\n\n\n\n## Load Packages, DON'T EDIT {#sec-packages -}\n\n### Install Missing {-}\n\nAny missing package will be installed automatically. This ensure smoother execution when run by others.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Do NOT modify\ninstall.packages(setdiff(packages, rownames(installed.packages())))\n```\n:::\n\n\n\n### Load {-}\n\nLoad all packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Do NOT modify\nlapply(packages, require, character.only = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] TRUE\n\n[[5]]\n[1] TRUE\n\n[[6]]\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n\n\n## Introduction\n\nAn attempt to understand Sherlock Holmes short stories found in Adventures of Sherlock Holmes book by Arthur Conan Doyle.  After inspecting the table of content, the book seems to have 12 stories, one story per chapter.  The analysis is inspired by [Julia Silge](https://juliasilge.com/)’s YouTube video [Topic modeling with R and tidy data principles](https://www.youtube.com/embed/evTuL-RcRpc)\n\n## Download Book\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download the book, each line of the book is read into a seperate row\nsherlock_raw <- gutenberg_download(48320)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nDetermining mirror for Project Gutenberg from https://www.gutenberg.org/robot/harvest\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUsing mirror http://aleph.gutenberg.org\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(sherlock_raw)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 12350     2\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(sherlock_raw)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 2\n  gutenberg_id text                           \n         <int> <chr>                          \n1        48320 \"ADVENTURES OF SHERLOCK HOLMES\"\n2        48320 \"\"                             \n3        48320 \"\"                             \n4        48320 \"\"                             \n5        48320 \"\"                             \n6        48320 \"[Illustration:\"               \n```\n\n\n:::\n\n```{.r .cell-code}\ntail(sherlock_raw)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 x 2\n  gutenberg_id text                                                  \n         <int> <chr>                                                 \n1        48320 \"  boisterious fashion, and on the whole _changed to_\"\n2        48320 \"  boisterous fashion, and on the whole\"              \n3        48320 \"\"                                                    \n4        48320 \"  Page 297\"                                          \n5        48320 \"  wrapt in the peaceful beauty _changed to_\"         \n6        48320 \"  rapt in the peaceful beauty\"                       \n```\n\n\n:::\n:::\n\n\n\n## Wrangle: Label Stories\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsherlock <- sherlock_raw %>% \n  \n  # determine start of each story/chapter\n  mutate(story = ifelse(str_detect(text, \"^(A SCANDAL IN BOHEMIA|THE RED-HEADED LEAGUE|A CASE OF IDENTITY|THE BOSCOMBE VALLEY MYSTERY|THE FIVE ORANGE PIPS|THE MAN WITH THE TWISTED LIP|THE ADVENTURE OF THE BLUE CARBUNCLE|THE ADVENTURE OF THE SPECKLED BAND|THE ADVENTURE OF THE ENGINEER’S THUMB|THE ADVENTURE OF THE NOBLE BACHELOR|THE ADVENTURE OF THE BERYL CORONET|THE ADVENTURE OF THE COPPER BEECHES)$\"), text, NA)) %>%\n  \n  # determine lines belonging to each story/chapter by\n  # filling down the N/A rows of story column\n  fill(story) %>%\n  \n  # remove the part that does not belong to any story/chapter,\n  # i.e, the introduction\n  filter(!is.na(story)) %>%\n  \n  # convert story column to factor\n  mutate(story = factor(story))\n```\n:::\n\n\n\n\n## Wrangle: Put in Tidy Format\n\nThe row of `text` column contains multiple words/tokens.  We want to put each word/token of each `text` row into a separate row.  This makes the dataframe follows the tidy format and hence makes it easy to process.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_sherlock <- sherlock %>%\n  \n  # number the rows\n  mutate(line = row_number()) %>% \n  \n  # break the text column into multiple row where each row contain one token\n  unnest_tokens(word, text) %>% \n  \n  # remove the stopwords--the rows where the word column is a stopword\n  anti_join(stop_words) %>% \n  \n  # remove holmes rows which might affect our topic models\n  filter(word != \"holmes\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining with `by = join_by(word)`\n```\n\n\n:::\n:::\n\n\n\n\n\n## Explore tf-idf\n\n-   To see which words are important in each story/chapter, i.e.,the words that appears many times in that story but few or none in the other stories.\n-   tf-idf (term frequency-inverse document frequency) is a great exploratory tool before starting with topic modeling\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_sherlock %>% \n  \n  # count number of occurrence of words in stories\n  count(story, word, sort = TRUE) %>% \n  \n  # compute and add tf, idf, and tf_idf values for words\n  bind_tf_idf(word, story, n) %>% \n  \n  # group by story\n  group_by(story) %>% \n  \n  # take top 10 words of each story with highest tf_idf (last column)\n  top_n(10) %>% \n  \n  # unpack\n  ungroup() %>% \n  \n  # turn words into factors and order them based on their tf_idf values\n  # NOTE: This will not affect order the dataframe rows which is can be\n  #   done via the arrange function\n  # NOTE: Recording the word column this way is for ggplot to visualize them\n  #   as desired from top tf_idf to lowest\n  mutate(word = reorder(word, tf_idf)) %>% \n  \n  # plot\n  ggplot(aes(word, tf_idf, fill = story)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~story, scales = \"free\", ncol = 3) +\n  theme(strip.text.x = element_text(size = 5)) +\n  coord_flip()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSelecting by tf_idf\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](topic-modeling-r_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Implement Topic Modeling\n\nTraining the model for the topics\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert from tidy form to quanteda form (document x term matrix)\nsherlock_stm <- tidy_sherlock %>% \n  count(story, word, sort = TRUE) %>% \n  cast_dfm(story, word, n)\n\n# Train the model\ntopic_model <- stm(sherlock_stm, K=6, init.type = \"Spectral\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBeginning Spectral Initialization \n\t Calculating the gram matrix...\n\t Finding anchor words...\n \t......\n\t Recovering initialization...\n \t.............................................................................\nInitialization complete.\n............\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 1 (approx. per word bound = -7.785) \n............\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 2 (approx. per word bound = -7.593, relative change = 2.458e-02) \n............\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 3 (approx. per word bound = -7.481, relative change = 1.473e-02) \n............\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 4 (approx. per word bound = -7.455, relative change = 3.469e-03) \n............\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 5 (approx. per word bound = -7.450, relative change = 7.612e-04) \nTopic 1: st, simon, lord, day, lady \n Topic 2: door, miss, house, rucastle, matter \n Topic 3: hat, goose, stone, bird, geese \n Topic 4: father, time, mccarthy, son, hand \n Topic 5: house, time, night, door, heard \n Topic 6: red, time, wilson, business, headed \n............\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 6 (approx. per word bound = -7.449, relative change = 1.233e-04) \n............\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nCompleting Iteration 7 (approx. per word bound = -7.449, relative change = 1.168e-05) \n............\nCompleted E-Step (0 seconds). \nCompleted M-Step. \nModel Converged \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(topic_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nA topic model with 6 topics, 12 documents and a 7709 word dictionary.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTopic 1 Top Words:\n \t Highest Prob: st, simon, lord, day, lady, found, matter \n \t FREX: simon, clair, neville, lascar, opium, doran, flora \n \t Lift: aloysius, ceremony, doran, millar, 2_s, aberdeen, absurdly \n \t Score: simon, st, clair, neville, _danseuse_, lestrade, doran \nTopic 2 Top Words:\n \t Highest Prob: door, miss, house, rucastle, matter, street, lady \n \t FREX: rucastle, hosmer, hunter, angel, windibank, _changed, 1 \n \t Lift: advertised, angel, annoyance, brothers, employed, factor, fowler \n \t Score: rucastle, hosmer, angel, windibank, hunter, type, 1 \nTopic 3 Top Words:\n \t Highest Prob: hat, goose, stone, bird, geese, baker, sir \n \t FREX: geese, horner, ryder, henry, peterson, salesman, countess \n \t Lift: battered, bet, bred, brixton, cosmopolitan, covent, cream \n \t Score: goose, geese, horner, _alias_, ryder, henry, peterson \nTopic 4 Top Words:\n \t Highest Prob: father, time, mccarthy, son, hand, lestrade, left \n \t FREX: mccarthy, pool, boscombe, openshaw, pips, horsham, turner \n \t Lift: bone, dundee, horsham, pondicherry, presumption, savannah, sundial \n \t Score: mccarthy, pool, lestrade, boscombe, openshaw, _détour_, turner \nTopic 5 Top Words:\n \t Highest Prob: house, time, night, door, heard, hand, round \n \t FREX: coronet, stoner, arthur, roylott, ventilator, gems, stoke \n \t Lift: _absolute_, _all_, _en, 1100, 16a, 3d, 4000 \n \t Score: coronet, arthur, stoner, gems, 4000, roylott, ventilator \nTopic 6 Top Words:\n \t Highest Prob: red, time, wilson, business, headed, day, league \n \t FREX: wilson, league, merryweather, jones, coburg, jabez, headed \n \t Lift: daring, saturday, vincent, _employé_, _october, _partie, 17 \n \t Score: wilson, league, merryweather, _employé_, jones, headed, coburg \n```\n\n\n:::\n:::\n\n\n\n## Contribution of Words in Topics\n\nLooking at which words contribute the most in each topic.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extracting betas and putting them in a tidy format\ntm_beta <- tidy(topic_model)\n\n# Visualizing the top words contributing to each topic\ntm_beta %>% \n  group_by(topic) %>% \n  # top 10 word in each topic with higest beta (last column)\n  top_n(10) %>% \n  ungroup() %>% \n  # turn words into factors and order them based on their tf_idf values\n  # NOTE: This will not affect order the dataframe rows which is can be\n  #   done via the arrange function\n  # NOTE: Recording the word column this way is for ggplot to visualize them\n  #   as desired from top tf_idf to lowest\n  mutate(term = reorder(term, beta)) %>% \n  ggplot(aes(term, beta, fill = topic)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~topic, scales = \"free\", ncol = 3) +\n  coord_flip()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nSelecting by beta\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](topic-modeling-r_files/figure-pdf/unnamed-chunk-9-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n## Distribution of Topics in Stories\n\nLooking at how the stories are associated with each topic and how strong each association is.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extracting gammas and putting them in a tidy format\ntm_gamma <- tidy(topic_model, matrix = \"gamma\",\n                 # use the names of the stories instead of the default numbers\n                 document_names = rownames(sherlock_stm))\n\n\n# Visualizing the number of stories belonging to each topics and the confidence\n#   of the belonging\ntm_gamma %>% \n  ggplot(aes(gamma, fill = as.factor(topic))) +\n  geom_histogram(show.legend = FALSE) +\n  facet_wrap(~topic, ncol = 3)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](topic-modeling-r_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualizing how much each topic appear in each story\ntm_gamma %>% \n  ggplot(aes(topic, gamma, fill = document)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~document, scales = \"free\", ncol = 3) +\n  theme(strip.text.x = element_text(size = 5))\n```\n\n::: {.cell-output-display}\n![](topic-modeling-r_files/figure-pdf/unnamed-chunk-11-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nThe model did an excellent job strongly associating the stories into one or more topics. This perfect association is rare in the world of topic modeling. The reason behind this perfect association here could be due to the small number of documents that we have.\n\n\n## References\n\n- Adventures of Sherlock Holmes book by Arthur Conan Doyle on [Project Gutenberg](https://www.gutenberg.org/ebooks/48320)\n- [Regular Expressions 101](https://regex101.com/)",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}